---
title: "R Notebook"
output: html_notebook
---

# Testing the Martingale hypothesis
Introduction: 

On se donne $\{ùëå_ùë° \}_{ùë°=‚àí‚àû}^{+‚àû}$ une s√©rie temporelle stationnaire et $ùêº_ùë°={ùëå_ùë°, ‚Ä¶,ùëå_0 }$ l‚Äôinformation disponible √† la date ùë°

# MDS
$ùëå_ùë°$ d√©finit une s√©quence de diff√©rences de martingales (MDS) si :

$$ ùîº[ùëå_ùë° |ùêº_{ùë°‚àí1} ]=0 $$
# MDH
Plus g√©n√©ralement on dit que $ùëå_ùë°$ satisfait l‚Äôhypoth√®se de diff√©rences de martingales (MDH)  si :
$$ ùîº[ùëå_ùë° |ùêº_{ùë°‚àí1} ]=ùúá‚àà‚Ñù~~~~~~~~~~~~~~(2)$$
Intuitivement cette hypoth√®se indique que le pr√©sent et le pass√© ne donnent pas plus d‚Äôinformation sur le futur que la moyenne du processus $ùíÄ_ùíï$ elle-m√™me ($ùîº[ùëå_ùë° ]=ùúá$). Autrement, dit que le meilleur pr√©dicteur du futur au sens des moindres carr√©s est $ùúá$.


La propri√©t√© fondamentale d√©coulant de la MDH est que $ùëå_ùë°$ est non pr√©dictible au sens des moindres carr√©s pour toute transformation lin√©aire ou non lin√©aire de l‚Äôinformation pass√©e $ùúî(ùêº_{ùë°‚àí1})$, i.e. de covariance nulle :
$$ ùîº[ùëå_ùë° |ùêº_{ùë°‚àí1} ]=ùúá‚àà‚Ñù‚Üîùîº[(ùëå_ùë°‚àíùúá)ùúî(ùêº_{ùë°‚àí1})]=0$$
En particulier, on note que toutes les autocorr√©lations (lag >0) de $ùëå_ùë°$ sont nulles. 

Rappelez-vous la d√©finition de mds dans l'√©quation (2) qui devrait tenir pour toute fonction w(). L'approche la plus simple consiste √† consid√©rer les fonctions lin√©aires w() ; telles que $ w(I_{t-1})= Y_{t-j}$ pour tout $j \geq 1$
Par cons√©quent, une condition n√©cessaire (mais pas suffisante, en g√©n√©ral) pour que la MDH tienne est que les s√©ries ne soient pas corr√©l√©es, c'est-√†-dire

$$ \gamma_j= Cov(Y_t,Y_{t-j})=ùîº[(ùëå_ùë°‚àíùúá)Y_{t-j}]=0~~~~~~~\forall j \geq1 $$

```{r}
rm(list=ls()) # Removes all existing variables

```



# parseur: 
```{r}
Sample = read.table("Economic_Scenarios_1000_Simu.csv",sep=";",header=FALSE) # This command returns an object of the class data.frame
 # For our purpose it is better to convert it into a matrix
n1 = dim(Sample)[1] # Number of rows
p1 = dim(Sample)[2] # Number of columns

X1_char = Sample[3:n1,7:p1] 
tab_X_1 = as.matrix(X1_char)

#pour la premiere simulation 

L=c()
for (i in 1:(floor(n1/4))){
  L=append(L,4*i)
}

IMMO=tab_X_1[L,]
EQUITY= tab_X_1[(L-1),]
INFL=tab_X_1[L-2,]
VALN=tab_X_1[L-3,]

```

#      I/ Test based on linear  measures of dependance
#        1/ Test based on a finite-dimensional conditioning set 

Supposons que nous observions des donn√©es brutes ${Y_t}_{t=1}^n $ alors $\hat{\gamma}_j$ est un estimateur consistant de $\gamma_j$: 

$$ \hat{\gamma}_j = (n-j)^{-1} \sum_{t=1+j}^{n}(Y_t-\bar{Y})(Y_{t-j}-\bar{Y})$$ 
$\bar{Y}$ est la moyenne de l'√©chantillon, on introduit aussi $\hat{\rho}_j$ le j eme ordre d'autocorr√©lation.


Premier test: 
$$ \hat{Q_p} = n\sum_{j=1}^p \hat{\rho}_j^2~~~~~~~avec ~~~~~ \hat{\rho}_j=\frac{\hat{\gamma}_j}{\hat{\gamma}_0}$$
Deuxieme test: 

$$\hat{Q_p^*} = n\sum_{j=1}^p \frac{\hat{\rho}_j^2}{\tau_j }$$
$$avec ~~~~~ \tau_j= \frac{1}{\hat{\gamma}_0^2}\sum_{j=1}^n (Y_t-\bar{Y})^2(Y_{t-j}-\bar{Y})^2 $$ 
Troisi√®me test - Variance Ratio: 

$$ \hat{VR_p}= 1+2\sum_{j=1}^{p-1}(1-\frac{j}{p})\hat{\rho}_j $$



```{r}
 #on actualise les donn√©es 

IMMO_act=IMMO*VALN
EQUITY_act = EQUITY*VALN 
INFL_act = INFL*VALN

DtSt_IMMO=IMMO*VALN #on actualise les donn√©es 
Dt_1St_1_IMMO=DtSt_IMMO[,1:(dim(DtSt_IMMO)[2]-1)]
Y_IMMO=DtSt_IMMO[,2:dim(DtSt_IMMO)[2]]-Dt_1St_1_IMMO

DtSt_EQUITY=EQUITY_act #on actualise les donn√©es 
Dt_1St_1_EQUITY=DtSt_EQUITY[,1:(dim(DtSt_EQUITY)[2]-1)]
Y_EQUITY=DtSt_EQUITY[,2:dim(DtSt_EQUITY)[2]]-Dt_1St_1_EQUITY


DtSt_INFL=INFL*VALN #on actualise les donn√©es 
Dt_1St_1_INFL=DtSt_INFL[,1:(dim(DtSt_INFL)[2]-1)]
Y_INFL=DtSt_INFL[,2:dim(DtSt_INFL)[2]]-Dt_1St_1_INFL


```

```{r}

mean((IMMO_act[1,] - mean(IMMO_act[1,]))^2)

```



```{r}


########################### Calcul de QP #########################

Qp= function(Sample, p) {
  n=dim(Sample)[1] #n =1000
  d=dim(Sample)[2] # d = 50

  Qp = c()
  
  for (i in 1:n) {
    
    Y_barre = mean(Sample[i,])
    gamma_0 = mean((Sample[i,]-Y_barre)^2)
    Qp_i = 0
    
    for (j in 1:p){
      gamma_j = mean((Sample[i,(1+j):d]-Y_barre)*(Sample[i,1:(d-j)]-Y_barre))
      rho_j = gamma_j/gamma_0
      Qp_i = Qp_i + d*(rho_j^2)
    }
    Qp = append(Qp, Qp_i)
    
  }
  return(Qp)

}

#print(Qp(IMMO_act,3))


########################### Calcul de QP* #########################

Qpstar = function(Sample, p) {
  n=dim(Sample)[1] #n =1000
  d=dim(Sample)[2] # d = 50

  Qpstar = c()
  
  for (i in 1:n) {
    
    Y_barre = mean(Sample[i,])
    gamma_0 = mean((Sample[i,]-Y_barre)^2)
    Qpstar_i = 0
    
    for (j in 1:p){
      gamma_j = mean((Sample[i,(1+j):d]-Y_barre)*(Sample[i,1:(d-j)]-Y_barre))
      
      rho_j = gamma_j/gamma_0
      
      tau_j = sum((Sample[i,(1+j):d]-Y_barre)^2*(Sample[i,1:(d-j)])^2) /(gamma_0^2)
      Qpstar_i = Qpstar_i + d*(rho_j^2)
    }
    Qpstar = append(Qpstar, Qpstar_i)
  }
  return(Qpstar)

}

########################### Calcul de VRp #########################


VRP= function(Sample, p) {
  n=dim(Sample)[1] #n =1000
  d=dim(Sample)[2] # d = 50

  VRP = c()
  
  for (i in 1:n) {
    Y_barre = mean(Sample[i,])
    gamma_0 = mean((Sample[i,]-Y_barre)^2)
    
    VRP_i = 1
    
    for (j in 1:(p-1)){
      gamma_j = mean((Sample[i,(1+j):d]-Y_barre)*(Sample[i,1:(d-j)]))
      rho_j = gamma_j/gamma_0
      VRP_i = VRP_i + (1-(j/p))*rho_j

      }
    VRP = append(VRP, VRP_i)
    
  }
  return(VRP)

}





```

Test statistique: 

on definit le degr√©e $\alpha$ 
l'hypoth√®se nulle est: $H_0 = \{Qp=0\}$ 
l'hypoth√®se alternantive est: $ H_1 = \{ |Q_p| \geq q_{chisq}(1-\alpha) \}$


```{r}

Graph = function(Sample,p){
  Qp = Qp(Sample,p)
  Y=c()
  for(t in 1:length(Qp)){
    p_value= 1-pchisq(Qp[t],df=p)
    Y=append(Y,p_value)
  }
  #X=c(1:length(Qp))
  
  #plot(X,Y)
  #abline(h=min(Y), col="blue")
  #abline(h=max(Y), col="blue")
  #abline(h=mean(Y), col="red")
  hist(Y,breaks=20,col="red",density=5,xlab="p-valeur",ylab="r√©partition",main=" R√©partition des p_valeurs ",tck=0.01)
    # breaks : nombre de barres
    # density : barres vides (0) ou hachur√©es
    # tck = 0.01 : longueur des graduations
    # xlab & ylab : titre de l'axe des abscisses et ordonn√©es
    # main : titre de l'histogramme
    # col : couleur des barres - pour mettre d'autres couleurs
    # Le param√®tre freq = FALSE ou freq = F permet d'afficher la fr√©quence que repr√©sente chaque cat√©gorie.
  box() # encadre l'histogramme
  #densite <- density(Y) # estimer la densit√© que repr√©sente ces diff√©rentes valeurs
  #lines(densite, col = "blue",lwd=2) # Superposer une ligne de densit√© √† l'histogramme
  
  print(paste0(" min(p_valeur)  = ", min(Y)))
  print(paste0(" max(p_valeur) = ",max(Y)))
  print(paste0(" mean(p_valeur) ",mean(Y)))
  print( "--------------")
  for(alpha in c(0.1,0.05,0.01)){
    
    print(paste0(" alpha = ",alpha))
    print(paste0(" On rejette l'hypoth√®se nulle ,",length(Y[Y<=alpha])))
    print( "--------------")   
  }
  
  
}
Graph(Y_IMMO,4)



```
On va essayer de construire les intervalles de confiances √† 95% pour les differents tests: 
Pour le test numero 1 et 2 on passe par la methode du delta: 


Pour le test numero 3 on connait deja un intervalle de confiance √† 95% sous hypoth√®se nulle : 
$$ I_3=\mathopen{[} VR_p - \phi_{1-\alpha/2}\sqrt{\frac{2*(p-1)}{p*n}},VR_p + \phi_{1-\alpha/2}\sqrt{\frac{2*(p-1)}{p*n}} \mathopen{]} $$
comme $\sqrt{n}*(VR_p-1)\longrightarrow \mathcal{N}(0,\frac{2*(p-1)}{p}) $ on a alors: 

$$I_3= \mathopen{[} 1 - \phi_{1-\alpha/2}\sqrt{\frac{2*(p-1)}{p*n}},1 + \phi_{1-\alpha/2}\sqrt{\frac{2*(p-1)}{p*n}} \mathopen{]} $$ 
```{r}

p=4
VR_P=VRP(Y_EQUITY,p)
X=c(1:length(VR_P))
print(paste0(" alpha = ",0.05))
a= 1-qnorm(0.975,mean=0,sd = 1)*sqrt(2*(p-1)/(p*length(VR_P)))
b= 1+qnorm(0.975,mean=0,sd = 1)*sqrt(2*(p-1)/(p*length(VR_P)))
#plot(X,VR_P)
hist(VR_P,breaks=20,col="red",density=6,xlab="Valeur de VR_P ",ylab="r√©partition",main=" R√©partition des valeurs ",tck=0.02)
abline(v=a,col="blue")
abline(v=b,col="blue")
box()

```




#        2/ Test based on a infinite-dimensional conditioning set 

