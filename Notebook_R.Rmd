---
title: "R Notebook"
output: html_notebook
---

# Testing the Martingale hypothesis
Introduction: 

On se donne $\{ğ‘Œ_ğ‘¡ \}_{ğ‘¡=âˆ’âˆ}^{+âˆ}$ une sÃ©rie temporelle stationnaire et $ğ¼_ğ‘¡={ğ‘Œ_ğ‘¡, â€¦,ğ‘Œ_0 }$ lâ€™information disponible Ã  la date ğ‘¡

# MDS
$ğ‘Œ_ğ‘¡$ dÃ©finit une sÃ©quence de diffÃ©rences de martingales (MDS) si :

$$ ğ”¼[ğ‘Œ_ğ‘¡ |ğ¼_{ğ‘¡âˆ’1} ]=0 $$
# MDH
Plus gÃ©nÃ©ralement on dit que $ğ‘Œ_ğ‘¡$ satisfait lâ€™hypothÃ¨se de diffÃ©rences de martingales (MDH)  si :
$$ ğ”¼[ğ‘Œ_ğ‘¡ |ğ¼_{ğ‘¡âˆ’1} ]=ğœ‡âˆˆâ„~~~~~~~~~~~~~~(2)$$
Intuitivement cette hypothÃ¨se indique que le prÃ©sent et le passÃ© ne donnent pas plus dâ€™information sur le futur que la moyenne du processus $ğ’€_ğ’•$ elle-mÃªme ($ğ”¼[ğ‘Œ_ğ‘¡ ]=ğœ‡$). Autrement, dit que le meilleur prÃ©dicteur du futur au sens des moindres carrÃ©s est $ğœ‡$.


La propriÃ©tÃ© fondamentale dÃ©coulant de la MDH est que $ğ‘Œ_ğ‘¡$ est non prÃ©dictible au sens des moindres carrÃ©s pour toute transformation linÃ©aire ou non linÃ©aire de lâ€™information passÃ©e $ğœ”(ğ¼_{ğ‘¡âˆ’1})$, i.e. de covariance nulle :
$$ ğ”¼[ğ‘Œ_ğ‘¡ |ğ¼_{ğ‘¡âˆ’1} ]=ğœ‡âˆˆâ„â†”ğ”¼[(ğ‘Œ_ğ‘¡âˆ’ğœ‡)ğœ”(ğ¼_{ğ‘¡âˆ’1})]=0$$
En particulier, on note que toutes les autocorrÃ©lations (lag >0) de $ğ‘Œ_ğ‘¡$ sont nulles. 

Rappelez-vous la dÃ©finition de mds dans l'Ã©quation (2) qui devrait tenir pour toute fonction w(). L'approche la plus simple consiste Ã  considÃ©rer les fonctions linÃ©aires w() ; telles que $ w(I_{t-1})= Y_{t-j}$ pour tout $j \geq 1$
Par consÃ©quent, une condition nÃ©cessaire (mais pas suffisante, en gÃ©nÃ©ral) pour que la MDH tienne est que les sÃ©ries ne soient pas corrÃ©lÃ©es, c'est-Ã -dire

$$ \gamma_j= Cov(Y_t,Y_{t-j})=ğ”¼[(ğ‘Œ_ğ‘¡âˆ’ğœ‡)Y_{t-j}]=0~~~~~~~\forall j \geq1 $$

```{r}
rm(list=ls()) # Removes all existing variables

```



# parseur: 
```{r}
Sample = read.table("Economic_Scenarios_1000_Simu.csv",sep=";",header=FALSE) # This command returns an object of the class data.frame
tab_X_1 = as.matrix(Sample) # For our purpose it is better to convert it into a matrix
n1 = dim(tab_X_1)[1] # Number of rows
p1 = dim(tab_X_1)[2] # Number of columns
X1_char = tab_X_1[3:n1,7:p1] 
n=dim(X1_char)[1] # Number of rows
p=dim(X1_char)[2] # Number of columns

#pour la premiere simulation 

VALN=c()
INFLN=c()
Equity= c()
IMMO=c()

for (k in 1:p){
  VALN<-c(VALN,as.numeric(X1_char[1,k]))
  INFLN<-c(INFLN,as.numeric(X1_char[2,k]))
  Equity<-c(Equity,as.numeric(X1_char[3,k]))
  IMMO<-c(IMMO,as.numeric(X1_char[4,k]))
}

#


#vÃ©rification 
IMMO[1]
IMMO[2]
IMMO[3]

```

#      I/ Test based on linear  measures of dependance
#        1/ Test based on a finite-dimensional conditioning set 

Supposons que nous observions des donnÃ©es brutes ${Y_t}_{t=1}^n $ alors $\hat{\gamma}_j$ est un estimateur consistant de $\gamma_j$: 

$$ \hat{\gamma}_j = (n-j)^{-1} \sum_{t=1+j}^{n}(Y_t-\bar{Y})(Y_{t-j}-\bar{Y})$$ 
$\bar{Y}$ est la moyenne de l'Ã©chantillon, on introduit aussi $\hat{\rho}_j$ le j eme ordre d'autocorrÃ©lation.


Premier test: 
$$ Q_p = n\sum_{j=1}^p \hat{\rho}_j^2~~~~~~~avec ~~~~~ \hat{\rho}_j=\frac{\hat{\gamma}_j}{\hat{\gamma}_0}$$
Deuxieme test: 

$$Q_p^* = n\sum_{j=1}^p \frac{\hat{\rho}_j^2}{\tau_j }$$
$$avec ~~~~~ \tau_j= \frac{1}{\hat{\gamma}_0^2}\sum_{j=1}^n (Y_t-\bar{Y})^2(Y_{t-j}-\bar{Y})^2 $$ 
TroisiÃ¨me test - Variance Ratio: 

$$ VR_p= 1+2\sum_{j=1}^{p-1}(1-\frac{j}{p})\hat{\rho}_j $$



```{r}
Y=IMMO*VALN #on actualise les donnÃ©es 

Y_barre= mean(Y)
p=3

########################### Calcul de QP #########################
liste_rho_j_carre=c()

#calcule de Gamma0 
gamma_0=0
for (t in 1:length(Y) ){
  gamma_0=gamma_0 + (Y[t]-Y_barre)*(Y[t]-Y_barre)/n
}

for (j in 1:p){
  gamma_j=0
  for (t in (1+j):length(Y) ){
    gamma_j=gamma_j + (Y[t]-Y_barre)*(Y[t-j]-Y_barre)/(n-j)
  }
  liste_rho_j_carre=c(liste_rho_j_carre,gamma_j**2/gamma_0**2)
}

Qp=n*sum(liste_rho_j_carre)
print(paste0(" Qp = ",Qp))

########################### Calcul de QP* #########################
liste_inverse_de_tau_j=c()

for (j in 1:p){
  tau_j=0
  for (t in (1+j):length(Y) ){
    
    tau_j=tau_j + ((Y[t]-Y_barre)**2)*(Y[t-j]-Y_barre)**2/(n-j)
  }
  liste_inverse_de_tau_j=c(liste_inverse_de_tau_j,gamma_0**2/tau_j)
}

Qp_etoile=n*sum(liste_inverse_de_tau_j*liste_rho_j_carre)
print(paste0(" Qp* = ",Qp_etoile))

########################### Calcul de VRp #########################
coeff= c()
for (j in 1:(p-1)){
  coeff=c(coeff,1-j/p)
}
VRp=1+2*sum(coeff*sqrt(liste_rho_j_carre[1:p-1]))
print(paste0(" VRp = ",VRp))

```

Test statistique: 

on definit le degrÃ©e $\alpha$ 
l'hypothÃ¨se nulle est: $H_0 = \{Qp=0\}$ 
l'hypothÃ¨se alternantive est: $ H_1 = \{ |Q_p| \geq q_{chisq}(1-\alpha) \}$


```{r}
for (alpha in c(0.1,0.05,0.01)){
  quantile_Qp = qchisq(1-alpha,df=p)
  p_value= 1-pchisq(quantile_Qp,df=p)
  print(paste0(" alpha = ", alpha))
  print(paste0(" Qp = ",Qp))
  print(paste0(" Quantile = ",quantile_Qp))
  print(paste0(" p-valeur = ",p_value))
  print("  " )
}



```





#        2/ Test based on a infinite-dimensional conditioning set 

