---
title: "R Notebook"
output: html_notebook
---

# Testing the Martingale hypothesis
Introduction: 

On se donne $\{ùëå_ùë° \}_{ùë°=‚àí‚àû}^{+‚àû}$ une s√©rie temporelle stationnaire et $ùêº_ùë°={ùëå_ùë°, ‚Ä¶,ùëå_0 }$ l‚Äôinformation disponible √† la date ùë°

# MDS
$ùëå_ùë°$ d√©finit une s√©quence de diff√©rences de martingales (MDS) si :

$$ ùîº[ùëå_ùë° |ùêº_{ùë°‚àí1} ]=0 $$
# MDH
Plus g√©n√©ralement on dit que $ùëå_ùë°$ satisfait l‚Äôhypoth√®se de diff√©rences de martingales (MDH)  si :
$$ ùîº[ùëå_ùë° |ùêº_{ùë°‚àí1} ]=ùúá‚àà‚Ñù~~~~~~~~~~~~~~(2)$$
Intuitivement cette hypoth√®se indique que le pr√©sent et le pass√© ne donnent pas plus d‚Äôinformation sur le futur que la moyenne du processus $ùíÄ_ùíï$ elle-m√™me ($ùîº[ùëå_ùë° ]=ùúá$). Autrement, dit que le meilleur pr√©dicteur du futur au sens des moindres carr√©s est $ùúá$.


La propri√©t√© fondamentale d√©coulant de la MDH est que $ùëå_ùë°$ est non pr√©dictible au sens des moindres carr√©s pour toute transformation lin√©aire ou non lin√©aire de l‚Äôinformation pass√©e $ùúî(ùêº_{ùë°‚àí1})$, i.e. de covariance nulle :
$$ ùîº[ùëå_ùë° |ùêº_{ùë°‚àí1} ]=ùúá‚àà‚Ñù‚Üîùîº[(ùëå_ùë°‚àíùúá)ùúî(ùêº_{ùë°‚àí1})]=0$$
En particulier, on note que toutes les autocorr√©lations (lag >0) de $ùëå_ùë°$ sont nulles. 

Rappelez-vous la d√©finition de mds dans l'√©quation (2) qui devrait tenir pour toute fonction w(). L'approche la plus simple consiste √† consid√©rer les fonctions lin√©aires w() ; telles que $ w(I_{t-1})= Y_{t-j}$ pour tout $j \geq 1$
Par cons√©quent, une condition n√©cessaire (mais pas suffisante, en g√©n√©ral) pour que la MDH tienne est que les s√©ries ne soient pas corr√©l√©es, c'est-√†-dire

$$ \gamma_j= Cov(Y_t,Y_{t-j})=ùîº[(ùëå_ùë°‚àíùúá)(Y_{t-j}-ùúá)]=0~~~~~~~\forall j \geq1 $$

```{r}
rm(list=ls()) # Removes all existing variables

```



# parseur: 
```{r}
Sample = read.table("Economic_Scenarios_1000_Simu.csv",sep=";",header=FALSE) # This command returns an object of the class data.frame
 # For our purpose it is better to convert it into a matrix
n1 = dim(Sample)[1] # Number of rows
p1 = dim(Sample)[2] # Number of columns

X1_char = Sample[3:n1,7:p1] 
tab_X_1 = as.matrix(X1_char)

#pour la premiere simulation 

L=c()
for (i in 1:(floor(n1/4))){
  L=append(L,4*i)
}

IMMO=tab_X_1[L,]
EQUITY= tab_X_1[(L-1),]
INFL=tab_X_1[L-2,]
VALN=tab_X_1[L-3,]

```

#      I/ Test based on linear  measures of dependance
#        1/ Test based on a finite-dimensional conditioning set 

Supposons que nous observions des donn√©es brutes ${Y_t}_{t=1}^n $ alors $\hat{\gamma}_j$ est un estimateur consistant de $\gamma_j$: 

$$ \hat{\gamma}_j = (n-j)^{-1} \sum_{t=1+j}^{n}(Y_t-\bar{Y})(Y_{t-j}-\bar{Y})$$ 
$\bar{Y}$ est la moyenne de l'√©chantillon, on introduit aussi $\hat{\rho}_j$ le j eme ordre d'autocorr√©lation.


Premier test: 
$$ \hat{Q_p} = n\sum_{j=1}^p \hat{\rho}_j^2~~~~~~~avec ~~~~~ \hat{\rho}_j=\frac{\hat{\gamma}_j}{\hat{\gamma}_0}$$
Deuxieme test: 

$$\hat{Q_p^*} = n\sum_{j=1}^p \frac{\hat{\rho}_j^2}{\tau_j }$$
$$avec ~~~~~ \tau_j= \frac{1}{\hat{\gamma}_0^2}\sum_{t=1+j}^n (Y_t-\bar{Y})^2(Y_{t-j}-\bar{Y})^2 $$ 
Troisi√®me test - Variance Ratio: 

$$ \hat{VR_p}= 1+2\sum_{j=1}^{p-1}(1-\frac{j}{p})\hat{\rho}_j $$
Quatri√®me test - Ljung-Box : 

$$ \hat{LB_p} = n(n+2)\sum_{j=1}^p \frac{\hat{\rho}_j^2}{n-j} $$


```{r}
 #on actualise les donn√©es 

IMMO_act=IMMO*VALN
EQUITY_act = EQUITY*VALN 
INFL_act = INFL*VALN

DtSt_IMMO=IMMO*VALN #on actualise les donn√©es 
Dt_1St_1_IMMO=DtSt_IMMO[,1:(dim(DtSt_IMMO)[2]-1)]
Y_IMMO=DtSt_IMMO[,2:dim(DtSt_IMMO)[2]]-Dt_1St_1_IMMO

DtSt_EQUITY=EQUITY_act #on actualise les donn√©es 
Dt_1St_1_EQUITY=DtSt_EQUITY[,1:(dim(DtSt_EQUITY)[2]-1)]
Y_EQUITY=DtSt_EQUITY[,2:dim(DtSt_EQUITY)[2]]-Dt_1St_1_EQUITY


DtSt_INFL=INFL*VALN #on actualise les donn√©es 
Dt_1St_1_INFL=DtSt_INFL[,1:(dim(DtSt_INFL)[2]-1)]
Y_INFL=DtSt_INFL[,2:dim(DtSt_INFL)[2]]-Dt_1St_1_INFL


```

```{r}

mean((IMMO_act[1,] - mean(IMMO_act[1,]))^2)

```



```{r}


########################### Calcul de QP #########################

Qp= function(Sample, p) {
  n=dim(Sample)[1] #n =1000
  d=dim(Sample)[2] # d = 50

  Qp = c()
  
  for (i in 1:n) {
    
    Y_barre = mean(Sample[i,])
    #gamma_0 = mean((Sample[i,]-Y_barre)^2)
    gamma_0 = mean((Sample[i,]-Y_barre)^2)
    Qp_i = 0
    
    for (j in 1:p){
      #gamma_j = mean((Sample[i,(1+j):d]-Y_barre)*(Sample[i,1:(d-j)]-Y_barre))
      gamma_j = mean((Sample[i,(1+j):d]-Y_barre)*(Sample[i,1:(d-j)]-Y_barre))
      rho_j = gamma_j/gamma_0
      
      Qp_i = Qp_i + d*(rho_j^2)
    }
    Qp = append(Qp, Qp_i)
    
  }
  return(Qp)

}

#print(Qp(IMMO_act,3))


########################### Calcul de QP* #########################

Qpstar = function(Sample, p) {
  n=dim(Sample)[1] #n =1000
  d=dim(Sample)[2] # d = 50

  Qpstar = c()
  
  for (i in 1:n) {
    
    Y_barre = mean(Sample[i,])
    gamma_0 = mean((Sample[i,]-Y_barre)^2)
    Qpstar_i = 0
    
    for (j in 1:p){
      gamma_j = mean((Sample[i,(1+j):d]-Y_barre)*(Sample[i,1:(d-j)]-Y_barre))
      
      rho_j = gamma_j/gamma_0
      
      
      tau_j = mean((Sample[i,(1+j):d]-Y_barre)^2*(Sample[i,1:(d-j)]-Y_barre)^2)/(gamma_0^2) #je pense que l'article √† oubli√© un facteur 1/(n-j) (si on enleve ce coefficient on obtient des valeurs absurdes)
      Qpstar_i = Qpstar_i + d*rho_j^2/tau_j
    }
    Qpstar = append(Qpstar, Qpstar_i)
  }
  return(Qpstar)

}

########################### Calcul de VRp #########################

VRP= function(Sample, p) {
  n=dim(Sample)[1] #n =1000
  d=dim(Sample)[2] # d = 50

  VRP = c()
  
  for (i in 1:n) {
    Y_barre = mean(Sample[i,])
    gamma_0 = mean((Sample[i,]-Y_barre)^2)
    
    VRP_i = 1
    for (j in 1:(p-1)){
      gamma_j = mean((Sample[i,(1+j):d]-Y_barre)*(Sample[i,1:(d-j)]-Y_barre))
      rho_j = gamma_j/gamma_0
      VRP_i = VRP_i +  2*(1-(j/p))*rho_j

      }
    VRP = append(VRP, VRP_i)
    
  }
  return(VRP)

}

################ Calcul Ljung and Box ####################

LB= function(Sample, p) {
  n=dim(Sample)[1] #n =1000
  d=dim(Sample)[2] # d = 50

  LB = c()
  
  for (i in 1:n) {
    
    Y_barre = mean(Sample[i,])
    #gamma_0 = mean((Sample[i,]-Y_barre)^2)
    gamma_0 = mean((Sample[i,])^2)
    LB_i = 0
    
    for (j in 1:p){
      #gamma_j = mean((Sample[i,(1+j):d]-Y_barre)*(Sample[i,1:(d-j)]-Y_barre))
      gamma_j = mean((Sample[i,(1+j):d])*(Sample[i,1:(d-j)]))
      rho_j = gamma_j/gamma_0
      LB_i = LB_i + (d+2)*(rho_j^2)*d/(d-j)
    }
    LB = append(LB, LB_i)
    
  }
  return(LB)

}




```

Test statistique: 

on definit le degr√©e $\alpha$ 
l'hypoth√®se nulle est: $H_0 = \{Qp=0\}$ 
l'hypoth√®se alternantive est: $ H_1 = \{ |Q_p| \geq q_{chisq}(1-\alpha) \}$


```{r}

Graph = function(Sample,p){
  Qp = Qp(Sample,p)
  Y=c()
  for(t in 1:length(Qp)){
    p_value= 1-pchisq(Qp[t],df=p)
    Y=append(Y,p_value)
  }
  #X=c(1:length(Qp))
  
  #plot(X,Y)
  #abline(h=min(Y), col="blue")
  #abline(h=max(Y), col="blue")
  #abline(h=mean(Y), col="red")
  hist(Y,breaks=20,col="red",density=5,xlab="p-valeur",ylab="r√©partition",main=" R√©partition des p_valeurs ",tck=0.01, freq = FALSE)
    # breaks : nombre de barres
    # density : barres vides (0) ou hachur√©es
    # tck = 0.01 : longueur des graduations
    # xlab & ylab : titre de l'axe des abscisses et ordonn√©es
    # main : titre de l'histogramme
    # col : couleur des barres - pour mettre d'autres couleurs
    # Le param√®tre freq = FALSE ou freq = F permet d'afficher la fr√©quence que repr√©sente chaque cat√©gorie.
  box() # encadre l'histogramme
  densite <- density(Y) # estimer la densit√© que repr√©sente ces diff√©rentes valeurs
  lines(densite, col = "blue",lwd=2) # Superposer une ligne de densit√© √† l'histogramme
  
  print(paste0(" min(p_valeur)  = ", min(Y)))
  print(paste0(" max(p_valeur) = ",max(Y)))
  print(paste0(" mean(p_valeur) ",mean(Y)))
  print( "--------------")
  for(alpha in c(0.1,0.05,0.01)){
    
    print(paste0(" alpha = ",alpha))
    print(paste0(" On rejette l'hypoth√®se nulle ,",length(Y[Y<=alpha])))
    print( "--------------")   
  }
  
  
}
Graph(Y_IMMO,2)



```
On va essayer de construire les intervalles de confiances √† 95% pour les differents tests: 
Pour le test numero 1 et 2 on passe par la methode du delta: 


Pour le test numero 3 on connait deja un intervalle de confiance √† 95% sous hypoth√®se nulle : 
$$ I_3=\mathopen{[} VR_p - \phi_{1-\alpha/2}\sqrt{\frac{2*(p-1)}{p*n}},VR_p + \phi_{1-\alpha/2}\sqrt{\frac{2*(p-1)}{p*n}} \mathopen{]} $$
comme $\sqrt{n}*(VR_p-1)\longrightarrow \mathcal{N}(0,\frac{2*(p-1)}{p}) $ on a alors: 

$$I_3= \mathopen{[} 1 - \phi_{1-\alpha/2}\sqrt{\frac{2*(p-1)}{p*n}},1 + \phi_{1-\alpha/2}\sqrt{\frac{2*(p-1)}{p*n}} \mathopen{]} $$ 
```{r}

p=4
VR_P=VRP(Y_IMMO,p)
X=c(1:length(VR_P))
print(paste0(" alpha = ",0.05))
a= 1-qnorm(0.975,mean=0,sd = 1)*sqrt(2*(p-1)/(p*dim(Y_IMMO)[2]))
b= 1+qnorm(0.975,mean=0,sd = 1)*sqrt(2*(p-1)/(p*dim(Y_IMMO)[2]))
#plot(X,VR_P)
print(length(VR_P))
hist(VR_P,breaks=20,col="red",density=6,xlab="Valeur de VR_P ",ylab="r√©partition",main=" R√©partition des valeurs ",tck=0.02)
abline(v=a,col="blue")
abline(v=b,col="blue")
abline(v=mean(VR_P),col="green")
box()
mean(VR_P)

```


#        1.bis / Calcul de la puissance 

Premier test pour controler la martingalit√©: 

$$ \frac{1}{N}*\sum_{i=1}^{N} D_t^i*S_t^i \longrightarrow S(0)  $$

$$ Y_t= X_{t+1}-X_t~~avec ~~~X_t=D_t*S_t $$ 
On sait qu'on a: 

On s'int√©rresse √† une classe de processus qui s'ecrit: 

$$ Y_{t+1} = \theta Y_t + \epsilon_t ~~~~o√π ~~~~ Y_0 = \frac{c}{1-\theta} ~~~ et~~~~ \epsilon_t~~ \longrightarrow ~~\mathcal{N}(c,\sigma^2)~~~~et~~~\theta \in [0,1[$$
Par recurrence on montre que: 
$$ Y_t=\theta^tY_{0}+\sum_{k=0}^{t-1}\theta^k\varepsilon_{t-k} $$ 

on a :
$$ \operatorname{E}[Y_t]=\theta^t Y_0 + c\sum_{i=0}^{t-1}\theta^i $$ 
$$ \operatorname{Var}[Y_t]= \sum_{i=0}^{t}\theta^{2i}\sigma^2 $$ 
$$\operatorname{Cov}[Y_t,Y_{t-j}]= \theta^{j}\sum_{i=0}^{t-j}\theta^{2i}\sigma^2 $$ 

donc on obtient: 

$$ \rho_j = \operatorname{Corr}[Y_t,Y_{t-j}]\equiv \frac{\operatorname{Cov}[Y_t,Y_{t-j}]}{\sqrt{\operatorname{Var}(Y_t)\operatorname{Var}(Y_{t-j})}}=\theta^j \sqrt{\frac{1 - \theta^{2(t-j) + 2}}{1 - \theta^{2t + 2}}} $$ 
le test portemanteau $Q_p$ s'int√©resse seulement aux $\rho_j$ pour $j\in [0,4]$. Comme $ \theta \in [0,1[$, si on prend $t$ assez grand (50) on obtient que : 

$$ \rho_j =\operatorname{Corr}[Y_t,Y_{t-j}]= \theta^j$$


#  G√©naration de ces processus pour diff√©rents \theta




```{r}
n=1000 #nombre de simulations
t_max=100
c=0
sigma = 5
p=4

simultheta = function(theta, t_max) {
  simul = c(rnorm(1,0, sigma^2/(1-theta^2)))
  bruit = rnorm(t_max-1, 0, sigma^2)
  
  for (i in (1:(t_max-1))) {
    
    X_i = theta*simul[i] + bruit[i]
    simul = append(simul, X_i)
  }
  return(t(matrix(simul)))
}

N_simul = function(theta, t_max, N){
  mat = matrix(0, ncol=t_max, nrow=N)
  for (i in 1:N){
    
    mat[i,] = simultheta(theta, t_max)
  }
  return(mat)
}



sums = function(Sample, theta, t_max){
  n = (dim(Sample))[1]
  for (i in 1:n){
   Sample[i,]= cumsum(Sample[i,])
  }
  sum = matrix(0, ncol=1, nrow = t_max)
  ak= c(0)
  bk= c(sigma**2)
  for (j in (1:t_max)){
    sum[j] = mean(Sample[,j])*sqrt(n/j)/sigma 
    
    
    
  }
  sum[1]=0
  return(sum)
}
theorie_vs_pratique = function(p){
  theta = seq(from = 0.01, to = 0.99, by = 0.01)
  n = length(theta)
  
  Qp_theo= c()
  Qps = c()
  for (thet in theta){
    simul = N_simul(thet, t_max, 1)
    
    Qp_theo_j=0
    Qps = c(Qps,Qp(simul, p))
    for(j in (1:p)){
      Qp_theo_j = Qp_theo_j + t_max*thet^(2*j)
    }
    Qp_theo=c(Qp_theo,Qp_theo_j)
    
  }
  
  plot(theta, Qps,col = "blue", type = "l", xlab = "Th√©ta", ylab = "Qp pour un mod√®le AR(1)")
  lines(theta, Qp_theo, col="red")
  legend(0, 350, legend=c("Qp experimentale","Qp th√©orique "),
       col=c( "blue","red"),lty=1:1, cex=0.8)
}
theorie_vs_pratique(4)
```

```{r}

```

```{r}

graph_power = function(n){
  theta = seq(from = 0.01, to = 0.99, by = 0.01)
  pwr1 = c()
  pwr2 = c()
  pwr3 = c()
  pwr4 = c()
  for (thet in theta) {
    simul = N_simul(thet, t_max, n)
    
    Qps = Qp(simul, p)
    VRP_s = sqrt(p*t_max)*(VRP(simul, p)-1)/(sqrt(2*(p-1)))
    Qpstar= Qpstar(simul, p)
    #LB = LB(simul, p)
    
    #sum = abs(sums(simul, thet, t_max))
    #print(sum)
    occ1 = length(Qps[Qps>qchisq(0.95,p)])/n
    #occ2 = length(sum[sum>qnorm(0.975)])/n #comparaison avec le test utilis√© par le milliman
    occ2 = length(VRP_s[abs(VRP_s)>qnorm(0.975)])/n # comparaison avec le test VRP 
    occ3 = length(Qpstar[Qpstar>qchisq(0.95,p)])/n 
   # occ4 = length(LB[LB>qchisq(0.95,p)])/n   
    
    pwr1 = append(pwr1, occ1)
    pwr2 = append(pwr2, occ2)
    pwr3 = append(pwr3, occ3)
   # pwr4 = append(pwr4, occ4)
    
    
  }
  plot(theta, pwr1, col="red", type = "l")
  lines(theta, pwr2, col="blue")
  lines(theta, pwr3, col="green") 
  #lines(theta, pwr4, col="orange")
  #abline(h=min(pwr), col="green")
  print(pwr1[1])
  print(pwr2[1])
  print(pwr3[1])
  #print(pwr4[1])
}

```



```{r}
graph_power(1000)
```


```{r}
simul = N_simul(0.5, t_max, 1)
VRP_s = sqrt(p*t_max/(2*(p-1)))*(VRP(simul, p)-1)
print(VRP_s)
```


```{r}
list = matrix(c(0.5,0.5,0.5))
print(sum(list^2))
n=1000 #nombre de simulations
t_max=50
c=0
sigma = 1
p=4
k = length(list)
sigma^2/(1-sum(list^2))
simul = c(rnorm(k, mean = 0, sd = sigma^2/(1-sum(list^2))))
simul
bruit = rnorm(t_max, c, sigma^2)
list*simul[(1):(k)]

x_i = sum(list*simul[(1):(k)]) + bruit[k+1]
simul = append(simul, x_i)

length(simultheta2(list, 50))
print(IMMO[1])
```

Puissance de Qp en fonction 
```{r}


n=1000 #nombre de simulations
t_max=50
c=0
sigma = 1
p=4
#k=4

simultheta2 = function(list , t_max) {
  k = dim(list)[1]
  simul = c(rnorm(k, mean = 0, sd = sqrt(sigma^2/(1-sum(list^2)))))
  bruit = rnorm(t_max, c, sigma^2)
  for (i in ((k+1):t_max)) {
    #print(k)
    X_i = sum(list*simul[(i-k):(i-1)]) + bruit[i]
    simul = append(simul, X_i)
  }
  return(t(matrix(simul)))
}
N_simul2 = function(list, t_max, N){
  mat = matrix(0, ncol=t_max, nrow=N)
  for (i in 1:N){
    mat[i,] = simultheta2(list, t_max)
  }
  return(mat)
}

```

```{r}
graph_power2 = function(n){
  theta = seq(from = 0.01, to = 1, by = 0.01)
  pwr1 = c()
  pwr2 = c()
  for (thet in theta) {
    #list = matrix(theta, nrow=1, ncol = k)
    list = matrix(c(0,0,0,0,0,0,0,0,thet))
    simul = N_simul(list, t_max, n)
    Qps = Qp(simul, p)
    
    #list2 = matrix(c(thet, thet,thet,thet))
    #simul2 = N_simul(list2, t_max, n)
    #Qps2 = Qp(simul2, p)
    #sum = abs(sums(simul, thet, t_max))
    #print(theta)
    occ1 = length(Qps[Qps>qchisq(0.95,p)])/n
    #occ2 = length(Qps[Qps2>qchisq(0.95,p)])/n
    #occ2 = length(sum[sum>qnorm(0.975)])/n
    pwr1 = append(pwr1, occ1)
    #pwr2 = append(pwr2, occ2)
    
  }
  plot(theta, pwr1, col="red",type = "l")
  #lines(theta, pwr2, col="blue")
  #abline(h=min(pwr), col="green")
  #print(min(pwr))
}
```

```{r}
graph_power2(1000)
```





```{r}

########################### Calcul de QP sur les colonnes #########################

Qpcol= function(Sample, p) {
  n=dim(Sample)[1] #n =1000
  d=dim(Sample)[2] # d = 50

 #valeur de t
  Qp = c()
  
  
  for (t in (p+1):(d)){
    Qp_t = 0
    Y_barre_t = mean(Sample[,t])
    gamma_0 = mean((Sample[,t]-Y_barre_t)^2)
    for (j in 1:p) {
    
      Y_barre_tmoinsj = mean(Sample[,t-j])
      
      gamma_j = mean((Sample[,t]-Y_barre_t)*(Sample[,(t-j)]-Y_barre_tmoinsj))
      
      rho_j = gamma_j/gamma_0
      
      #print(rho_j^2)  
      Qp_t = Qp_t + n*(rho_j^2)
    }
    Qp = append(Qp, Qp_t)
  }
  return(Qp)

}
```

```{r}

Graph = function(Sample,p){
  Qp = Qpcol(Sample,p)
  print(Qp)
  Y=c()
  for(t in 1:length(Qp)){
    p_value= 1-pchisq(Qp[t],df=p)
    Y=append(Y,p_value)
  }
  #X=c(1:length(Qp))
  
  #plot(X,Y)
  #abline(h=min(Y), col="blue")
  #abline(h=max(Y), col="blue")
  #abline(h=mean(Y), col="red")
  hist(Y,breaks=20,col="red",density=5,xlab="p-valeur",ylab="r√©partition",main=" R√©partition des p_valeurs ",tck=0.01)
    # breaks : nombre de barres
    # density : barres vides (0) ou hachur√©es
    # tck = 0.01 : longueur des graduations
    # xlab & ylab : titre de l'axe des abscisses et ordonn√©es
    # main : titre de l'histogramme
    # col : couleur des barres - pour mettre d'autres couleurs
    # Le param√®tre freq = FALSE ou freq = F permet d'afficher la fr√©quence que repr√©sente chaque cat√©gorie.
  box() # encadre l'histogramme
  #densite <- density(Y) # estimer la densit√© que repr√©sente ces diff√©rentes valeurs
  #lines(densite, col = "blue",lwd=2) # Superposer une ligne de densit√© √† l'histogramme
  
  print(paste0(" min(p_valeur)  = ", min(Y)))
  print(paste0(" max(p_valeur) = ",max(Y)))
  print(paste0(" mean(p_valeur) ",mean(Y)))
  print( "--------------")
  for(alpha in c(0.1,0.05,0.01)){
    
    print(paste0(" alpha = ",alpha))
    print(paste0(" On rejette l'hypoth√®se nulle ,",length(Y[Y<=alpha])))
    print( "--------------")   
  }
  
  
}
Graph(Y_IMMO,4)

```

```{r}
2**2
```





#        2/ Test based on a infinite-dimensional conditioning set 

La limite du test pr√©c√©dent est qu'on ne consid√®re seulement un nombre fini d'autocorr√©lations. On va donc utiliser la densit√© spectrale (qui contient toutes les covariances).

On d√©finit la densit√© spectrale $f(\lambda)$ par

$$ \gamma_k = \int_{\Pi}^{}f(\lambda)exp(ik\lambda)d\lambda  \\
k = 0,1,2,... \\
\Pi = [-\pi, \pi]$$

On d√©finit √©galement le p√©riodigramme par 

$$ I(\lambda) = \lvert w(\lambda) \rvert ^2 \\
\text{o√π } w(\lambda) = n^{-1/2} \sum_{t=1}^{n} x_t exp(it\lambda)$$

L'int√©grale de la densit√© spectrale est lin√©aire en $\lambda$ sous l'hypoth√®se de MDH.

On a alors l'hypoth√®se nulle

$$ H_0 = \{\gamma_k = 0,  \forall k \}$$

Ce qui est √©quivalent √†

$$ f(\lambda) = \{f(\lambda) = \frac{\gamma_0}{2\pi},  \forall \lambda \in \Pi \}$$

On introduit donc le p√©riodigramme cumulatif standardis√©

$$ Z_n(\lambda) = \sqrt{T} ( \frac{\sum_{j=1}^{[\lambda T  \pi]} I(\lambda_j)}{\sum_{j=1}^{T} I(\lambda_j)} - \frac{\lambda}{\pi} ) \\
\text{o√π } \lambda_j = \frac{2\pi j}{n} \\
j = 1,2,..., n/2$$

A partir de $Z_n$, on peut en d√©duire le test de Kolmogorov Smirnov

$$ \max_{j=1,...,T} \lvert Z_n(\lambda_j) \rvert$$
Et le test de Cramer von Mises

$$ \frac{1}{T} \sum_{j=1}^{T} Z_n(\lambda_j ^2)$$

On introduit la statistique de Deo (qui peut √™tre vue comme une version continue de la statistique $Q_p^*$). Elle s'√©crit

$$DEO_n = \sum_{j=1}^{n-1}n \frac{\hat{\rho}_j^2}{\tau_j}(\frac{1}{j \pi})^2$$

#        3/ S√©lection de p

```{r}
pi = function(p,n,q,all_rho){
  if (max(all_rho)<= sqrt(q*log(n)/n)){
    return (p*log(n))
  }else{return (2*p)}
}


Best_p = function(Sample){
n=dim(Sample)[1] #n =1000
d=dim(Sample)[2] # d = 50
q=2.4            # Inglot & Ledwina 2.4
all_rho_abs = c()
Y_barre = mean(Sample[n,])
gamma_0 = mean((Sample[n,]-Y_barre)^2)

for (j in 1:d-1){
    gamma_j = mean((Sample[n,(1+j):d]-Y_barre)*(Sample[n,1:(d-j)]-Y_barre))
    rho_j = gamma_j/gamma_0
    all_rho_abs = append(all_rho_abs,abs(rho_j))
  }
L = c()
for (p in 1:d-1){
  L_j = Qpstar(Sample,p)-pi(p,n,q,all_rho_abs)
  L = append(L,t(L_j))
}
Lmat= matrix(L,n,d)
print(dim(Lmat))




max_col <- argmax(Lmat,rows=TRUE)

nb_iter = c()
for (i in 1:d){
  nb_iter = append(nb_iter,sum(max_col==i))
  
}
return(which.max(nb_iter))
}

#Best_p(Y_EQUITY) #1 pour "n=n"
#Best_p(Y_IMMO)
#Best_p(Y_INFL)

```
```{r}
for (i in 1:10){
  print(i/10)
  print(Best_p(N_simul(i/10,50,1000)))
}

```
```{r}
for (i in 1:10){
  print(i*100)
  print(Best_p(N_simul(.5,50,i*100)))
}
```

```{r}
Best_p(simultheta(.75,50))
```


